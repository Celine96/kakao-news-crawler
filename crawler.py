"""
REXA ìë™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
- 5ë¶„ë¶„ë§ˆë‹¤ ì‹¤í–‰ (Cron Job)
- ë¶€ë™ì‚° ë‰´ìŠ¤ 20ê°œ ìˆ˜ì§‘ â†’ í•„í„°ë§ â†’ ì €ì¥
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime
from openai import OpenAI

# ê³µí†µ í•¨ìˆ˜ ì„í¬íŠ¸
from common import (
    search_naver_news,
    save_all_news_background,
    init_google_sheets,
    init_csv_file
)

# ================================================================================
# ë¡œê¹… ì„¤ì •
# ================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ================================================================================
# í¬ë¡¤ë§ í†µê³„
# ================================================================================

class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„ ì¶”ì """
    def __init__(self):
        self.total_fetched = 0
        self.total_filtered = 0
        self.total_saved = 0
        self.start_time = None
        self.end_time = None
    
    def print_summary(self):
        """í†µê³„ ìš”ì•½ ì¶œë ¥"""
        if self.start_time and self.end_time:
            elapsed = (self.end_time - self.start_time).total_seconds()
            logger.info("=" * 70)
            logger.info("ğŸ“Š í¬ë¡¤ë§ í†µê³„ ìš”ì•½")
            logger.info(f"   â±ï¸  ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
            logger.info(f"   ğŸ” ìˆ˜ì§‘: {self.total_fetched}ê°œ (ë„¤ì´ë²„ API)")
            logger.info(f"   âœ… í•„í„°ë§ í›„: {self.total_filtered}ê°œ (ë¶€ë™ì‚° ê´€ë ¨)")
            logger.info(f"   ğŸ’¾ ì €ì¥: {self.total_saved}ê°œ (êµ¬ê¸€ì‹œíŠ¸/CSV)")
            if self.total_fetched > 0:
                filter_rate = (self.total_filtered / self.total_fetched) * 100
                logger.info(f"   ğŸ“ˆ í•„í„°ë§ìœ¨: {filter_rate:.1f}%")
            logger.info("=" * 70)

# ================================================================================
# ë‰´ìŠ¤ ìš”ì•½ í•¨ìˆ˜ (í¬ë¡¤ëŸ¬ ì „ìš©)
# ================================================================================

def generate_news_summary(title: str, description: str) -> str:
    """
    GPTë¥¼ ì‚¬ìš©í•´ì„œ ë‰´ìŠ¤ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ (í¬ë¡¤ëŸ¬ ì „ìš©)
    
    Args:
        title: ë‰´ìŠ¤ ì œëª©
        description: ë„¤ì´ë²„ APIì—ì„œ ë°›ì€ description
    
    Returns:
        3-4ë¬¸ì¥ì˜ ì¶©ì‹¤í•œ ìš”ì•½ (200-250ì)
    """
    
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    if not OPENAI_API_KEY:
        # GPT ì‚¬ìš© ë¶ˆê°€ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        if len(description) > 250:
            sentences = description.split('.')
            if len(sentences) >= 3:
                return sentences[0] + '.' + sentences[1] + '.' + sentences[2] + '.'
            else:
                return description[:250].strip() + '...'
        return description
    
    system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë‰´ìŠ¤ ì œëª©ê³¼ ì„¤ëª…ì„ ì½ê³ , í•µì‹¬ ë‚´ìš©ì„ 3-4ë¬¸ì¥ìœ¼ë¡œ ì¶©ì‹¤í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.

ìš”ì•½ ê·œì¹™:
- 3-4ë¬¸ì¥ìœ¼ë¡œ ì‘ì„± (200-250ì)
- êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨ (ìˆ˜ì¹˜, ë‚ ì§œ, ì§€ì—­, ì£¼ì²´ ë“±)
- ë‹¨ìˆœíˆ "~í–ˆë‹¤"ê°€ ì•„ë‹ˆë¼ "ëˆ„ê°€, ë¬´ì—‡ì„, ì™œ, ì–´ë–»ê²Œ"ë¥¼ í¬í•¨
- ë‰´ìŠ¤ì˜ ë§¥ë½ê³¼ ë°°ê²½ê¹Œì§€ ê°„ëµíˆ ì„¤ëª…
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥

ì˜ˆì‹œ:
ì…ë ¥: "ì„œìš¸ ê°•ë‚¨êµ¬ ì¬ê±´ì¶• ì•„íŒŒíŠ¸ ê°€ê²© ê¸‰ë“±...ê·œì œ ì™„í™” ì˜í–¥"
ì¶œë ¥: "ì„œìš¸ ê°•ë‚¨êµ¬ ì¬ê±´ì¶• ì•„íŒŒíŠ¸ ê°€ê²©ì´ ì „ì›” ëŒ€ë¹„ 5% ìƒìŠ¹í–ˆë‹¤. ì •ë¶€ì˜ ì¬ê±´ì¶• ê·œì œ ì™„í™”ì™€ ê¸ˆë¦¬ ì¸í•˜ ê¸°ëŒ€ê°ì´ ì£¼ìš” ì›ì¸ìœ¼ë¡œ ì‘ìš©í–ˆë‹¤. íŠ¹íˆ ëŒ€ì¹˜ë™ê³¼ ì••êµ¬ì •ë™ ì¼ëŒ€ ë‹¨ì§€ë“¤ì´ ê°•ì„¸ë¥¼ ë³´ì˜€ë‹¤. ì „ë¬¸ê°€ë“¤ì€ ì´ëŸ¬í•œ ìƒìŠ¹ì„¸ê°€ ë‹¹ë¶„ê°„ ì§€ì†ë  ê²ƒìœ¼ë¡œ ì „ë§í–ˆë‹¤."

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{"summary": "ìš”ì•½ ë‚´ìš©"}"""

    user_prompt = f"""ì œëª©: {title}
ì„¤ëª…: {description}

ìœ„ ë‰´ìŠ¤ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ì¶©ì‹¤í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. êµ¬ì²´ì ì¸ ì •ë³´(ìˆ˜ì¹˜, ì§€ì—­, ë‚ ì§œ ë“±)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”."""

    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
            timeout=15
        )
        
        result = json.loads(response.choices[0].message.content)
        summary = result.get('summary', description[:250])
        
        # ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(summary) > 280:
            summary = summary[:277] + '...'
        
        return summary
        
    except Exception as e:
        logger.warning(f"âš ï¸ GPT ìš”ì•½ ì‹¤íŒ¨: {e} - ì›ë³¸ ì‚¬ìš©")
        # ì‹¤íŒ¨ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        if len(description) > 250:
            sentences = description.split('.')
            if len(sentences) >= 3:
                return sentences[0] + '.' + sentences[1] + '.' + sentences[2] + '.'
            else:
                return description[:250].strip() + '...'
        return description

# ================================================================================
# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ================================================================================

async def auto_crawl():
    """ìë™ í¬ë¡¤ë§ ë©”ì¸ ë¡œì§"""
    stats = CrawlStats()
    stats.start_time = datetime.now()
    
    logger.info("=" * 70)
    logger.info(f"â° ìë™ í¬ë¡¤ë§ ì‹œì‘: {stats.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)
    
    try:
        # 1. ì´ˆê¸°í™”
        logger.info("ğŸ”§ ì´ˆê¸°í™” ì¤‘...")
        csv_success = init_csv_file()
        gsheet_success = init_google_sheets()
        
        if csv_success:
            logger.info("   âœ… CSV ì´ˆê¸°í™” ì™„ë£Œ")
        if gsheet_success:
            logger.info("   âœ… Google Sheets ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. ë‰´ìŠ¤ ê²€ìƒ‰ (20ê°œ)
        logger.info("")
        logger.info("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        logger.info("   ê²€ìƒ‰ì–´: ë¶€ë™ì‚°")
        logger.info("   ìš”ì²­ ê°œìˆ˜: 20ê°œ")
        
        news_items = search_naver_news("ë¶€ë™ì‚°", display=20)
        
        if not news_items or len(news_items) == 0:
            logger.warning("")
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ")
            logger.warning("   ì›ì¸: ë„¤ì´ë²„ API ì˜¤ë¥˜ ë˜ëŠ” í•„í„°ë§ ê²°ê³¼ 0ê°œ")
            stats.end_time = datetime.now()
            stats.print_summary()
            return
        
        stats.total_fetched = 20  # ë„¤ì´ë²„ API ìš”ì²­ ê°œìˆ˜
        stats.total_filtered = len(news_items)  # í•„í„°ë§ í›„ ê°œìˆ˜
        
        logger.info("")
        logger.info(f"âœ… {len(news_items)}ê°œ ë¶€ë™ì‚° ê´€ë ¨ ë‰´ìŠ¤ ë°œê²¬")
        
        # ìƒìœ„ 3ê°œ ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°
        logger.info("")
        logger.info("ğŸ“° ìƒìœ„ 3ê°œ ë‰´ìŠ¤:")
        for idx, item in enumerate(news_items[:3]):
            logger.info(f"   [{idx+1}] {item['title'][:50]}...")
            logger.info(f"       ì ìˆ˜: {item.get('relevance_score', 0)}ì  | "
                       f"ì§€ì—­: {item.get('region', 'N/A')} | "
                       f"í‚¤ì›Œë“œ: {', '.join(item.get('keywords', [])[:3])}")
        
        # 3. ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (í¬ë¡¤ëŸ¬ ì „ìš©)
        logger.info("")
        logger.info("ğŸ“ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘...")
        for idx, item in enumerate(news_items):
            original_desc = item['description']
            summary = generate_news_summary(item['title'], original_desc)
            item['description'] = summary
            logger.info(f"   [{idx+1}/{len(news_items)}] ìš”ì•½ ì™„ë£Œ: {item['title'][:40]}...")
        
        # 4. ë°±ê·¸ë¼ìš´ë“œ ì €ì¥
        logger.info("")
        logger.info("ğŸ’¾ êµ¬ê¸€ ì‹œíŠ¸/CSV ì €ì¥ ì¤‘...")
        await save_all_news_background(news_items, user_id="auto_crawler")
        
        stats.total_saved = len(news_items)
        
        # 5. ì™„ë£Œ
        stats.end_time = datetime.now()
        logger.info("")
        logger.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        stats.print_summary()
        
    except KeyboardInterrupt:
        logger.info("")
        logger.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(0)
        
    except Exception as e:
        logger.error("")
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {type(e).__name__}")
        logger.error(f"   ì—ëŸ¬ ë©”ì‹œì§€: {e}")
        
        import traceback
        logger.error("")
        logger.error("ğŸ“‹ ìƒì„¸ ì—ëŸ¬ ë¡œê·¸:")
        for line in traceback.format_exc().split('\n'):
            if line.strip():
                logger.error(f"   {line}")
        
        stats.end_time = datetime.now()
        stats.print_summary()
        sys.exit(1)  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ ì½”ë“œ 1

# ================================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ================================================================================

if __name__ == "__main__":
    """
    ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Render Cron Jobìœ¼ë¡œ 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ë©ë‹ˆë‹¤.
    
    ë¡œì»¬ í…ŒìŠ¤íŠ¸:
        python crawler.py
    
    Render ì„¤ì • (render.yaml):
        schedule: "0 * * * *"  # ë§¤ì‹œ 0ë¶„ì— ì‹¤í–‰
    """
    
    try:
        asyncio.run(auto_crawl())
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)
