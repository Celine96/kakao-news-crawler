"""
REXA ìë™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
- 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (Cron Job)
- ë¶€ë™ì‚° ë‰´ìŠ¤ 20ê°œ ìˆ˜ì§‘ â†’ í•„í„°ë§ â†’ ì €ì¥
"""

import asyncio
import logging
import sys
from datetime import datetime

# ê³µí†µ í•¨ìˆ˜ ì„í¬íŠ¸
from common import (
    search_naver_news,
    save_all_news_background,
    init_google_sheets,
    init_csv_file
)

# ================================================================================
# ë¡œê¹… ì„¤ì •
# ================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ================================================================================
# í¬ë¡¤ë§ í†µê³„
# ================================================================================

class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„ ì¶”ì """
    def __init__(self):
        self.total_fetched = 0
        self.total_filtered = 0
        self.total_saved = 0
        self.start_time = None
        self.end_time = None
    
    def print_summary(self):
        """í†µê³„ ìš”ì•½ ì¶œë ¥"""
        if self.start_time and self.end_time:
            elapsed = (self.end_time - self.start_time).total_seconds()
            logger.info("=" * 70)
            logger.info("ğŸ“Š í¬ë¡¤ë§ í†µê³„ ìš”ì•½")
            logger.info(f"   â±ï¸  ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
            logger.info(f"   ğŸ” ìˆ˜ì§‘: {self.total_fetched}ê°œ (ë„¤ì´ë²„ API)")
            logger.info(f"   âœ… í•„í„°ë§ í›„: {self.total_filtered}ê°œ (ë¶€ë™ì‚° ê´€ë ¨)")
            logger.info(f"   ğŸ’¾ ì €ì¥: {self.total_saved}ê°œ (êµ¬ê¸€ì‹œíŠ¸/CSV)")
            if self.total_fetched > 0:
                filter_rate = (self.total_filtered / self.total_fetched) * 100
                logger.info(f"   ğŸ“ˆ í•„í„°ë§ìœ¨: {filter_rate:.1f}%")
            logger.info("=" * 70)

# ================================================================================
# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ================================================================================

async def auto_crawl():
    """ìë™ í¬ë¡¤ë§ ë©”ì¸ ë¡œì§"""
    stats = CrawlStats()
    stats.start_time = datetime.now()
    
    logger.info("=" * 70)
    logger.info(f"â° ìë™ í¬ë¡¤ë§ ì‹œì‘: {stats.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)
    
    try:
        # 1. ì´ˆê¸°í™”
        logger.info("ğŸ”§ ì´ˆê¸°í™” ì¤‘...")
        csv_success = init_csv_file()
        gsheet_success = init_google_sheets()
        
        if csv_success:
            logger.info("   âœ… CSV ì´ˆê¸°í™” ì™„ë£Œ")
        if gsheet_success:
            logger.info("   âœ… Google Sheets ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. ë‰´ìŠ¤ ê²€ìƒ‰ (20ê°œ)
        logger.info("")
        logger.info("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        logger.info("   ê²€ìƒ‰ì–´: ë¶€ë™ì‚°")
        logger.info("   ìš”ì²­ ê°œìˆ˜: 20ê°œ")
        
        news_items = search_naver_news("ë¶€ë™ì‚°", display=20)
        
        if not news_items or len(news_items) == 0:
            logger.warning("")
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ")
            logger.warning("   ì›ì¸: ë„¤ì´ë²„ API ì˜¤ë¥˜ ë˜ëŠ” í•„í„°ë§ ê²°ê³¼ 0ê°œ")
            stats.end_time = datetime.now()
            stats.print_summary()
            return
        
        stats.total_fetched = 20  # ë„¤ì´ë²„ API ìš”ì²­ ê°œìˆ˜
        stats.total_filtered = len(news_items)  # í•„í„°ë§ í›„ ê°œìˆ˜
        
        logger.info("")
        logger.info(f"âœ… {len(news_items)}ê°œ ë¶€ë™ì‚° ê´€ë ¨ ë‰´ìŠ¤ ë°œê²¬")
        
        # ìƒìœ„ 3ê°œ ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°
        logger.info("")
        logger.info("ğŸ“° ìƒìœ„ 3ê°œ ë‰´ìŠ¤:")
        for idx, item in enumerate(news_items[:3]):
            logger.info(f"   [{idx+1}] {item['title'][:50]}...")
            logger.info(f"       ì ìˆ˜: {item.get('relevance_score', 0)}ì  | "
                       f"ì§€ì—­: {item.get('region', 'N/A')} | "
                       f"í‚¤ì›Œë“œ: {', '.join(item.get('keywords', [])[:3])}")
        
        # 3. ë°±ê·¸ë¼ìš´ë“œ ì €ì¥
        logger.info("")
        logger.info("ğŸ’¾ êµ¬ê¸€ ì‹œíŠ¸/CSV ì €ì¥ ì¤‘...")
        await save_all_news_background(news_items, user_id="auto_crawler")
        
        stats.total_saved = len(news_items)
        
        # 4. ì™„ë£Œ
        stats.end_time = datetime.now()
        logger.info("")
        logger.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        stats.print_summary()
        
    except KeyboardInterrupt:
        logger.info("")
        logger.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(0)
        
    except Exception as e:
        logger.error("")
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {type(e).__name__}")
        logger.error(f"   ì—ëŸ¬ ë©”ì‹œì§€: {e}")
        
        import traceback
        logger.error("")
        logger.error("ğŸ“‹ ìƒì„¸ ì—ëŸ¬ ë¡œê·¸:")
        for line in traceback.format_exc().split('\n'):
            if line.strip():
                logger.error(f"   {line}")
        
        stats.end_time = datetime.now()
        stats.print_summary()
        sys.exit(1)  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ ì½”ë“œ 1

# ================================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ================================================================================

if __name__ == "__main__":
    """
    ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Render Cron Jobìœ¼ë¡œ 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ë©ë‹ˆë‹¤.
    
    ë¡œì»¬ í…ŒìŠ¤íŠ¸:
        python crawler.py
    
    Render ì„¤ì • (render.yaml):
        schedule: "0 * * * *"  # ë§¤ì‹œ 0ë¶„ì— ì‹¤í–‰
    """
    
    try:
        asyncio.run(auto_crawl())
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)
