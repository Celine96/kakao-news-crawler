"""
REXA ìë™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
- 5ë¶„ë§ˆë‹¤ ì‹¤í–‰ (Cron Job)
- ë¶€ë™ì‚° ë‰´ìŠ¤ 20ê°œ ìˆ˜ì§‘ â†’ í•„í„°ë§ â†’ ì €ì¥
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime
from openai import OpenAI
from difflib import SequenceMatcher

# ê³µí†µ í•¨ìˆ˜ ì„í¬íŠ¸
from common import (
    search_naver_news,
    save_all_news_background,
    init_google_sheets,
    init_csv_file,
    get_recent_urls_from_gsheet,
    get_recent_titles_from_gsheet
)

# ================================================================================
# ë¡œê¹… ì„¤ì •
# ================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ================================================================================
# í¬ë¡¤ë§ í†µê³„
# ================================================================================

class CrawlStats:
    """í¬ë¡¤ë§ í†µê³„ ì¶”ì """
    def __init__(self):
        self.total_fetched = 0
        self.total_filtered = 0
        self.total_saved = 0
        self.start_time = None
        self.end_time = None
    
    def print_summary(self):
        """í†µê³„ ìš”ì•½ ì¶œë ¥"""
        if self.start_time and self.end_time:
            elapsed = (self.end_time - self.start_time).total_seconds()
            logger.info("=" * 70)
            logger.info("ğŸ“Š í¬ë¡¤ë§ í†µê³„ ìš”ì•½")
            logger.info(f"   â±ï¸  ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
            logger.info(f"   ğŸ” ìˆ˜ì§‘: {self.total_fetched}ê°œ (ë„¤ì´ë²„ API)")
            logger.info(f"   âœ… í•„í„°ë§ í›„: {self.total_filtered}ê°œ (ë¶€ë™ì‚° ê´€ë ¨)")
            logger.info(f"   ğŸ’¾ ì €ì¥: {self.total_saved}ê°œ (êµ¬ê¸€ì‹œíŠ¸/CSV)")
            if self.total_fetched > 0:
                filter_rate = (self.total_filtered / self.total_fetched) * 100
                logger.info(f"   ğŸ“ˆ í•„í„°ë§ìœ¨: {filter_rate:.1f}%")
            logger.info("=" * 70)

# ================================================================================
# ë‰´ìŠ¤ ìš”ì•½ í•¨ìˆ˜ (í¬ë¡¤ëŸ¬ ì „ìš©)
# ================================================================================

def generate_news_summary(title: str, description: str) -> str:
    """
    GPTë¥¼ ì‚¬ìš©í•´ì„œ ë‰´ìŠ¤ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ (í¬ë¡¤ëŸ¬ ì „ìš©)
    
    Args:
        title: ë‰´ìŠ¤ ì œëª©
        description: ë„¤ì´ë²„ APIì—ì„œ ë°›ì€ description
    
    Returns:
        3-4ë¬¸ì¥ì˜ ì¶©ì‹¤í•œ ìš”ì•½ (200-250ì)
    """
    
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    if not OPENAI_API_KEY:
        # GPT ì‚¬ìš© ë¶ˆê°€ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        if len(description) > 250:
            sentences = description.split('.')
            if len(sentences) >= 3:
                return sentences[0] + '.' + sentences[1] + '.' + sentences[2] + '.'
            else:
                return description[:250].strip() + '...'
        return description
    
    system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë‰´ìŠ¤ ì œëª©ê³¼ ì„¤ëª…ì„ ì½ê³ , í•µì‹¬ ë‚´ìš©ì„ 3-4ë¬¸ì¥ìœ¼ë¡œ ì¶©ì‹¤í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.

ìš”ì•½ ê·œì¹™:
- 3-4ë¬¸ì¥ìœ¼ë¡œ ì‘ì„± (200-250ì)
- êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨ (ìˆ˜ì¹˜, ë‚ ì§œ, ì§€ì—­, ì£¼ì²´ ë“±)
- ë‹¨ìˆœíˆ "~í–ˆë‹¤"ê°€ ì•„ë‹ˆë¼ "ëˆ„ê°€, ë¬´ì—‡ì„, ì™œ, ì–´ë–»ê²Œ"ë¥¼ í¬í•¨
- ë‰´ìŠ¤ì˜ ë§¥ë½ê³¼ ë°°ê²½ê¹Œì§€ ê°„ëµíˆ ì„¤ëª…
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥

ì˜ˆì‹œ:
ì…ë ¥: "ì„œìš¸ ê°•ë‚¨êµ¬ ì¬ê±´ì¶• ì•„íŒŒíŠ¸ ê°€ê²© ê¸‰ë“±...ê·œì œ ì™„í™” ì˜í–¥"
ì¶œë ¥: "ì„œìš¸ ê°•ë‚¨êµ¬ ì¬ê±´ì¶• ì•„íŒŒíŠ¸ ê°€ê²©ì´ ì „ì›” ëŒ€ë¹„ 5% ìƒìŠ¹í–ˆë‹¤. ì •ë¶€ì˜ ì¬ê±´ì¶• ê·œì œ ì™„í™”ì™€ ê¸ˆë¦¬ ì¸í•˜ ê¸°ëŒ€ê°ì´ ì£¼ìš” ì›ì¸ìœ¼ë¡œ ì‘ìš©í–ˆë‹¤. íŠ¹íˆ ëŒ€ì¹˜ë™ê³¼ ì••êµ¬ì •ë™ ì¼ëŒ€ ë‹¨ì§€ë“¤ì´ ê°•ì„¸ë¥¼ ë³´ì˜€ë‹¤. ì „ë¬¸ê°€ë“¤ì€ ì´ëŸ¬í•œ ìƒìŠ¹ì„¸ê°€ ë‹¹ë¶„ê°„ ì§€ì†ë  ê²ƒìœ¼ë¡œ ì „ë§í–ˆë‹¤."

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{"summary": "ìš”ì•½ ë‚´ìš©"}"""

    user_prompt = f"""ì œëª©: {title}
ì„¤ëª…: {description}

ìœ„ ë‰´ìŠ¤ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ì¶©ì‹¤í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. êµ¬ì²´ì ì¸ ì •ë³´(ìˆ˜ì¹˜, ì§€ì—­, ë‚ ì§œ ë“±)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”."""

    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
            timeout=15
        )
        
        result = json.loads(response.choices[0].message.content)
        summary = result.get('summary', description[:250])
        
        # ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(summary) > 280:
            summary = summary[:277] + '...'
        
        return summary
        
    except Exception as e:
        logger.warning(f"âš ï¸ GPT ìš”ì•½ ì‹¤íŒ¨: {e} - ì›ë³¸ ì‚¬ìš©")
        # ì‹¤íŒ¨ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        if len(description) > 250:
            sentences = description.split('.')
            if len(sentences) >= 3:
                return sentences[0] + '.' + sentences[1] + '.' + sentences[2] + '.'
            else:
                return description[:250].strip() + '...'
        return description

def calculate_title_similarity(title1: str, title2: str) -> float:
    """
    ë‘ ì œëª© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)
    
    Args:
        title1: ì²« ë²ˆì§¸ ì œëª©
        title2: ë‘ ë²ˆì§¸ ì œëª©
    
    Returns:
        ìœ ì‚¬ë„ (0.0 = ì™„ì „ ë‹¤ë¦„, 1.0 = ì™„ì „ ê°™ìŒ)
    """
    # ì†Œë¬¸ì ë³€í™˜ ë° ê³µë°± ì •ë¦¬
    t1 = title1.lower().strip()
    t2 = title2.lower().strip()
    
    # SequenceMatcherë¡œ ìœ ì‚¬ë„ ê³„ì‚°
    similarity = SequenceMatcher(None, t1, t2).ratio()
    
    return similarity

def remove_duplicate_news(news_items: list, similarity_threshold: float = 0.75) -> list:
    """
    ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°
    
    Args:
        news_items: ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        similarity_threshold: ì¤‘ë³µ íŒë‹¨ ì„ê³„ê°’ (0.75 = 75% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ)
    
    Returns:
        ì¤‘ë³µì´ ì œê±°ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not news_items:
        return []
    
    # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_items = sorted(
        news_items, 
        key=lambda x: x.get('relevance_score', 0), 
        reverse=True
    )
    
    unique_news = []
    removed_count = 0
    
    for item in sorted_items:
        is_duplicate = False
        
        # ì´ë¯¸ ì„ íƒëœ ë‰´ìŠ¤ë“¤ê³¼ ë¹„êµ
        for selected in unique_news:
            similarity = calculate_title_similarity(
                item['title'], 
                selected['title']
            )
            
            if similarity >= similarity_threshold:
                is_duplicate = True
                removed_count += 1
                logger.info(
                    f"   âš ï¸ ì¤‘ë³µ ì œê±°: '{item['title'][:40]}...' "
                    f"(ìœ ì‚¬ë„: {similarity:.0%} with '{selected['title'][:30]}...')"
                )
                break
        
        if not is_duplicate:
            unique_news.append(item)
    
    logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(sorted_items)}ê°œ â†’ {len(unique_news)}ê°œ (ì¤‘ë³µ {removed_count}ê°œ ì œê±°)")
    
    return unique_news

# ================================================================================
# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ================================================================================

async def auto_crawl():
    """ìë™ í¬ë¡¤ë§ ë©”ì¸ ë¡œì§"""
    stats = CrawlStats()
    stats.start_time = datetime.now()
    
    logger.info("=" * 70)
    logger.info(f"â° ìë™ í¬ë¡¤ë§ ì‹œì‘: {stats.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 70)
    
    try:
        # 1. ì´ˆê¸°í™”
        logger.info("ğŸ”§ ì´ˆê¸°í™” ì¤‘...")
        csv_success = init_csv_file()
        gsheet_success = init_google_sheets()
        
        if csv_success:
            logger.info("   âœ… CSV ì´ˆê¸°í™” ì™„ë£Œ")
        if gsheet_success:
            logger.info("   âœ… Google Sheets ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. ë‰´ìŠ¤ ê²€ìƒ‰ (20ê°œ)
        logger.info("")
        logger.info("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        logger.info("   ê²€ìƒ‰ì–´: ë¶€ë™ì‚°")
        logger.info("   ìš”ì²­ ê°œìˆ˜: 20ê°œ")
        
        news_items = search_naver_news("ë¶€ë™ì‚°", display=20)
        
        if not news_items or len(news_items) == 0:
            logger.warning("")
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ")
            logger.warning("   ì›ì¸: ë„¤ì´ë²„ API ì˜¤ë¥˜ ë˜ëŠ” í•„í„°ë§ ê²°ê³¼ 0ê°œ")
            stats.end_time = datetime.now()
            stats.print_summary()
            return
        
        stats.total_fetched = 20  # ë„¤ì´ë²„ API ìš”ì²­ ê°œìˆ˜
        stats.total_filtered = len(news_items)  # í•„í„°ë§ í›„ ê°œìˆ˜
        
        logger.info("")
        logger.info(f"âœ… {len(news_items)}ê°œ ë¶€ë™ì‚° ê´€ë ¨ ë‰´ìŠ¤ ë°œê²¬")
        
        # ìƒìœ„ 3ê°œ ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°
        logger.info("")
        logger.info("ğŸ“° ìƒìœ„ 3ê°œ ë‰´ìŠ¤:")
        for idx, item in enumerate(news_items[:3]):
            logger.info(f"   [{idx+1}] {item['title'][:50]}...")
            logger.info(f"       ì ìˆ˜: {item.get('relevance_score', 0)}ì  | "
                       f"ì§€ì—­: {item.get('region', 'N/A')} | "
                       f"í‚¤ì›Œë“œ: {', '.join(item.get('keywords', [])[:3])}")
        
        # 3. ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (í¬ë¡¤ëŸ¬ ì „ìš©)
        logger.info("")
        logger.info("ğŸ“ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘...")
        for idx, item in enumerate(news_items):
            original_desc = item['description']
            summary = generate_news_summary(item['title'], original_desc)
            item['description'] = summary
            logger.info(f"   [{idx+1}/{len(news_items)}] ìš”ì•½ ì™„ë£Œ: {item['title'][:40]}...")
        
        # 4. ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜)
        logger.info("")
        logger.info("ğŸ” ì¤‘ë³µ ë‰´ìŠ¤ í™•ì¸ ì¤‘ (ì œëª© ìœ ì‚¬ë„)...")
        original_count = len(news_items)
        news_items = remove_duplicate_news(news_items, similarity_threshold=0.75)
        
        # ì¤‘ë³µ ì œê±° í›„ í†µê³„ ì—…ë°ì´íŠ¸
        if len(news_items) < original_count:
            logger.info(f"   ğŸ“Š ì¤‘ë³µ ì œê±°: {original_count}ê°œ â†’ {len(news_items)}ê°œ")
        else:
            logger.info(f"   âœ… ì¤‘ë³µ ì—†ìŒ: {len(news_items)}ê°œ ìœ ì§€")
        
        # 5. DB ì¤‘ë³µ ì²´í¬ (ìµœê·¼ 3ì‹œê°„ URL + ìµœê·¼ 24ì‹œê°„ ì œëª© í™•ì¸)
        logger.info("")
        logger.info("ğŸ” DB ì¤‘ë³µ í™•ì¸ ì¤‘ (URL + ì œëª©)...")
        
        # URL ì¤‘ë³µ ì²´í¬ (ìµœê·¼ 3ì‹œê°„)
        recent_urls = get_recent_urls_from_gsheet(hours=3)
        # ì œëª© ì¤‘ë³µ ì²´í¬ (ìµœê·¼ 24ì‹œê°„)
        recent_titles = get_recent_titles_from_gsheet(hours=24)
        
        before_db_check = len(news_items)
        new_news_items = []
        duplicate_count = 0
        
        for item in news_items:
            # URL ì²´í¬
            url = item.get('link') or item.get('url', '')
            if url and url in recent_urls:
                duplicate_count += 1
                logger.info(f"   âš ï¸ URL ì¤‘ë³µ: '{item['title'][:40]}...' (ì´ë¯¸ ì €ì¥ëœ URL)")
                continue
            
            # ì œëª© ì²´í¬ (ì •ê·œí™”)
            title = item.get('title', '')
            normalized_title = title.lower().strip().replace(' ', '')
            if normalized_title and normalized_title in recent_titles:
                duplicate_count += 1
                logger.info(f"   âš ï¸ ì œëª© ì¤‘ë³µ: '{item['title'][:40]}...' (ì´ë¯¸ ì €ì¥ëœ ì œëª©)")
                continue
            
            # ì¤‘ë³µì´ ì•„ë‹ˆë©´ ì¶”ê°€
            new_news_items.append(item)
        
        news_items = new_news_items
        
        if duplicate_count > 0:
            logger.info(f"   ğŸ“Š DB ì¤‘ë³µ ì œê±°: {before_db_check}ê°œ â†’ {len(news_items)}ê°œ (ì¤‘ë³µ {duplicate_count}ê°œ)")
        else:
            logger.info(f"   âœ… DB ì¤‘ë³µ ì—†ìŒ: {len(news_items)}ê°œ ëª¨ë‘ ì‹ ê·œ")
        
        # ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if len(news_items) == 0:
            logger.warning("")
            logger.warning("âš ï¸ ì €ì¥í•  ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŒ")
            logger.warning("   ì›ì¸: ëª¨ë‘ ìµœê·¼ 3ì‹œê°„ ë‚´ ì €ì¥ëœ ë‰´ìŠ¤")
            stats.end_time = datetime.now()
            stats.print_summary()
            return
        
        # 6. ë°±ê·¸ë¼ìš´ë“œ ì €ì¥
        logger.info("")
        logger.info("ğŸ’¾ êµ¬ê¸€ ì‹œíŠ¸/CSV ì €ì¥ ì¤‘...")
        await save_all_news_background(news_items, user_id="auto_crawler")
        
        stats.total_saved = len(news_items)
        
        # 7. ì™„ë£Œ
        stats.end_time = datetime.now()
        logger.info("")
        logger.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        stats.print_summary()
        
    except KeyboardInterrupt:
        logger.info("")
        logger.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(0)
        
    except Exception as e:
        logger.error("")
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {type(e).__name__}")
        logger.error(f"   ì—ëŸ¬ ë©”ì‹œì§€: {e}")
        
        import traceback
        logger.error("")
        logger.error("ğŸ“‹ ìƒì„¸ ì—ëŸ¬ ë¡œê·¸:")
        for line in traceback.format_exc().split('\n'):
            if line.strip():
                logger.error(f"   {line}")
        
        stats.end_time = datetime.now()
        stats.print_summary()
        sys.exit(1)  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ ì½”ë“œ 1

# ================================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ================================================================================

if __name__ == "__main__":
    """
    ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Render Cron Jobìœ¼ë¡œ 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ë©ë‹ˆë‹¤.
    
    ë¡œì»¬ í…ŒìŠ¤íŠ¸:
        python crawler.py
    
    Render ì„¤ì • (render.yaml):
        schedule: "0 * * * *"  # ë§¤ì‹œ 0ë¶„ì— ì‹¤í–‰
    """
    
    try:
        asyncio.run(auto_crawl())
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)
