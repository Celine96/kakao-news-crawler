"""
REXA ê³µí†µ í•¨ìˆ˜ ëª¨ë“ˆ
- ë‰´ìŠ¤ ê²€ìƒ‰/í•„í„°ë§
- í¬ë¡¤ë§
- ì €ì¥ (CSV, Google Sheets)
"""

import logging
import os
import time
import re
import csv
import json
from datetime import datetime
from typing import Optional
import asyncio

import requests
from bs4 import BeautifulSoup
from openai import OpenAI

# Google Sheetsìš©
try:
    import gspread
    from google.oauth2.service_account import Credentials
    GSPREAD_AVAILABLE = True
except ImportError:
    GSPREAD_AVAILABLE = False
    logging.warning("gspread not installed. Google Sheets logging disabled.")

# ================================================================================
# í™˜ê²½ë³€ìˆ˜
# ================================================================================

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_SHEETS_CREDENTIALS = os.getenv("GOOGLE_SHEETS_CREDENTIALS")
GOOGLE_SHEETS_SPREADSHEET_ID = os.getenv("GOOGLE_SHEETS_SPREADSHEET_ID")

CSV_FILE_PATH = "news_data.csv"

# ================================================================================
# ê¸€ë¡œë²Œ ë³€ìˆ˜
# ================================================================================

gsheet_client = None
gsheet_worksheet = None

logger = logging.getLogger(__name__)

# ================================================================================
# ë‰´ìŠ¤ í•„í„°ë§ ì‹œìŠ¤í…œ
# ================================================================================

def is_headline_news(title: str, description: str) -> bool:
    """
    í—¤ë“œë¼ì¸/ì¢…í•© ë‰´ìŠ¤ì¸ì§€ íŒë‹¨
    
    Returns:
        True: í—¤ë“œë¼ì¸ ë‰´ìŠ¤ (ì œì™¸ ëŒ€ìƒ)
        False: ì¼ë°˜ ë‰´ìŠ¤
    """
    text = (title + " " + description).lower()
    
    # í—¤ë“œë¼ì¸ íŒ¨í„´ í‚¤ì›Œë“œ
    headline_keywords = [
        "ì˜¤ëŠ˜ì˜ ë¶€ë™ì‚° ë‰´ìŠ¤",
        "ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤",
        "ë¶€ë™ì‚° ë‰´ìŠ¤ ì´ì •ë¦¬",
        "í—¤ë“œë¼ì¸",
        "ë‰´ìŠ¤ ë¸Œë¦¬í•‘",
        "ë‰´ìŠ¤ ëª¨ìŒ",
        "ì£¼ìš” ë‰´ìŠ¤",
        "ë‰´ìŠ¤ ì •ë¦¬",
    ]
    
    for keyword in headline_keywords:
        if keyword in text:
            return True
    
    # íŒ¨í„´ ë§¤ì¹­: "ë‰´ìŠ¤ (ì´ Nê±´)", "ì´ Nê±´ì˜ ë‰´ìŠ¤" ë“±
    patterns = [
        r'ë‰´ìŠ¤\s*\(ì´\s*\d+ê±´\)',  # ë‰´ìŠ¤ (ì´ 5ê±´)
        r'ì´\s*\d+ê±´',             # ì´ 5ê±´
        r'\d+ê±´ì˜?\s*ë‰´ìŠ¤',        # 5ê±´ì˜ ë‰´ìŠ¤
    ]
    
    for pattern in patterns:
        if re.search(pattern, text):
            return True
    
    return False


def check_celebrity_scandal(title: str, description: str) -> dict:
    """
    ì—°ì˜ˆì¸ ê´€ë ¨ ë‰´ìŠ¤ì˜ ë¶€ë™ì‚° ê´€ë ¨ì„± íŒë‹¨
    
    Returns:
        {
            'is_celebrity_news': bool,
            'should_exclude': bool,  # Trueë©´ ì œì™¸
            'reason': str
        }
    """
    text = (title + " " + description).lower()
    
    # ì—°ì˜ˆì¸ í‚¤ì›Œë“œ
    celebrity_keywords = [
        "ë°°ìš°", "ê°€ìˆ˜", "ì—°ì˜ˆì¸", "ì•„ì´ëŒ", "íƒ¤ëŸ°íŠ¸",
        "ìŠ¤íƒ€", "ì…€ëŸ½", "ë°©ì†¡ì¸", "ì½”ë¯¸ë””ì–¸", "ê°œê·¸ë§¨"
    ]
    
    # ë¶€ë™ì‚° ê±°ë˜ í‚¤ì›Œë“œ (í¬í•¨ OK)
    transaction_keywords = [
        "ë§¤ë§¤", "ë§¤ì…", "êµ¬ì…", "êµ¬ë§¤", "ì·¨ë“", "ìƒ€ë‹¤", "ì‚¬ë“¤",
        "ë§¤ë„", "íŒë§¤", "ì²˜ë¶„", "íŒ”ì•˜ë‹¤", "íŒ”ì•„",
        "ì–µì›ì—", "ì–µëŒ€", "ì–µì›ëŒ€",
        "íˆ¬ì", "ë¶„ì–‘", "ì…ì£¼",
        "ìƒˆì§‘", "ì´ì‚¬"
    ]
    
    # ë¶„ìŸ/ìŠ¤ìº”ë“¤ í‚¤ì›Œë“œ (ì œì™¸)
    scandal_keywords = [
        "ë¶„ìŸ", "ê°ˆë“±", "ì†Œì†¡", "ê³ ì†Œ", "ê³ ë°œ",
        "í˜ì˜", "ì˜í˜¹", "ë…¼ë€", "í­ë¡œ", "ê³ ë°œ",
        "ì‚¬ê¸°", "íš¡ë ¹", "ë°°ì„",
        "ì „ ë‚¨í¸", "ì „ ë¶€ì¸", "ì´í˜¼", "ìœ„ìë£Œ"
    ]
    
    is_celebrity = any(kw in text for kw in celebrity_keywords)
    has_transaction = any(kw in text for kw in transaction_keywords)
    has_scandal = any(kw in text for kw in scandal_keywords)
    
    # ì—°ì˜ˆì¸ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ íŒ¨ìŠ¤
    if not is_celebrity:
        return {
            'is_celebrity_news': False,
            'should_exclude': False,
            'reason': 'ì—°ì˜ˆì¸ ë‰´ìŠ¤ ì•„ë‹˜'
        }
    
    # ì—°ì˜ˆì¸ + ë¶„ìŸ/ìŠ¤ìº”ë“¤ = ì œì™¸
    if has_scandal:
        return {
            'is_celebrity_news': True,
            'should_exclude': True,
            'reason': 'ì—°ì˜ˆì¸ ë¶„ìŸ/ìŠ¤ìº”ë“¤ (ë¶€ë™ì‚° ê±°ë˜ ë¬´ê´€)'
        }
    
    # ì—°ì˜ˆì¸ + ê±°ë˜ í‚¤ì›Œë“œ = í¬í•¨
    if has_transaction:
        return {
            'is_celebrity_news': True,
            'should_exclude': False,
            'reason': 'ì—°ì˜ˆì¸ ë¶€ë™ì‚° ë§¤ìˆ˜/ë§¤ë„ (í¬í•¨)'
        }
    
    # ì• ë§¤í•œ ê²½ìš° - GPTê°€ íŒë‹¨í•˜ë„ë¡ ë„˜ê¹€
    return {
        'is_celebrity_news': True,
        'should_exclude': False,
        'reason': 'ì—°ì˜ˆì¸ ê´€ë ¨ì´ì§€ë§Œ ì¶”ê°€ íŒë‹¨ í•„ìš”'
    }


def filter_real_estate_news(title: str, description: str) -> dict:
    """
    ê¸°ì‚¬ê°€ ë¶€ë™ì‚°ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ GPTë¡œ íŒë‹¨í•˜ê³  í•µì‹¬ ì§€í‘œ ì¶”ì¶œ
    
    Returns:
        {
            'is_relevant': bool,
            'relevance_score': int,
            'keywords': list,
            'region': str or None,
            'has_price': bool,
            'has_policy': bool,
            'reason': str
        }
    """
    
    # ============================================================
    # 1ë‹¨ê³„: í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ì‚¬ì „ í•„í„°ë§
    # ============================================================
    if is_headline_news(title, description):
        logging.info(f"âŒ [í—¤ë“œë¼ì¸ ì œì™¸] {title[:50]}...")
        return {
            'is_relevant': False,
            'relevance_score': 0,
            'keywords': [],
            'region': None,
            'has_price': False,
            'has_policy': False,
            'reason': 'í—¤ë“œë¼ì¸/ì¢…í•© ë‰´ìŠ¤'
        }
    
    # ============================================================
    # 2ë‹¨ê³„: ì—°ì˜ˆì¸ ë¶„ìŸ ë‰´ìŠ¤ í•„í„°ë§
    # ============================================================
    celebrity_check = check_celebrity_scandal(title, description)
    if celebrity_check['should_exclude']:
        logging.info(f"âŒ [ì—°ì˜ˆì¸ ë¶„ìŸ ì œì™¸] {title[:50]}... ({celebrity_check['reason']})")
        return {
            'is_relevant': False,
            'relevance_score': 0,
            'keywords': [],
            'region': None,
            'has_price': False,
            'has_policy': False,
            'reason': celebrity_check['reason']
        }
    
    # ============================================================
    # 3ë‹¨ê³„: GPT í•„í„°ë§ (ê¸°ì¡´ ë¡œì§)
    # ============================================================
    if not OPENAI_API_KEY:
        logging.warning("âš ï¸ OPENAI_API_KEY not set - using keyword filtering")
        return filter_by_keywords(title, description)
    
    system_prompt = """ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë‰´ìŠ¤ í•„í„°ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª…ì„ ë³´ê³  ì´ê²ƒì´ "ë¶€ë™ì‚°ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€" íŒë‹¨í•˜ì„¸ìš”.

âœ… ë¶€ë™ì‚° ê´€ë ¨ ê¸°ì‚¬:
- ì•„íŒŒíŠ¸, ì˜¤í”¼ìŠ¤í…”, ìƒê°€, í† ì§€ ë“± ë¶€ë™ì‚° ë§¤ë§¤/ì„ëŒ€
- ë¶€ë™ì‚° ê°€ê²©, ì‹œì„¸, ê±°ë˜ëŸ‰
- ë¶€ë™ì‚° ì •ì±…, ì„¸ê¸ˆ, ëŒ€ì¶œ, ê¸ˆë¦¬
- ì¬ê±´ì¶•, ì¬ê°œë°œ, ë¶„ì–‘, ì²­ì•½
- ë¶€ë™ì‚° íˆ¬ì, ìˆ˜ìµí˜• ë¶€ë™ì‚°
- **ì—°ì˜ˆì¸ì˜ ë¶€ë™ì‚° ë§¤ìˆ˜/ë§¤ë„/íˆ¬ì (OK)**

âŒ ë¶€ë™ì‚° ë¬´ê´€ ê¸°ì‚¬:
- **í—¤ë“œë¼ì¸ ë‰´ìŠ¤, ì¢…í•© ë‰´ìŠ¤ (ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ëª¨ì€ ê²ƒ)**
- **ì—°ì˜ˆì¸ ë¶„ìŸ/ìŠ¤ìº”ë“¤ (ë¶€ë™ì‚° ê±°ë˜ì™€ ë¬´ê´€í•œ ì†Œì†¡, ê°ˆë“±)**
- ì£¼ì‹, ì±„ê¶Œ, ì½”ì¸ ë“± ê¸ˆìœµìƒí’ˆ
- ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ (ë¶€ë™ì‚° ì–¸ê¸‰ ì—†ìŒ)
- ì •ì¹˜, ì‚¬íšŒ, ë¬¸í™” ì´ìŠˆ
- ê±´ì„¤ì‚¬ ì‹¤ì ì´ì§€ë§Œ ë¶€ë™ì‚°ê³¼ ì§ì ‘ ì—°ê´€ ì—†ìŒ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "is_relevant": true/false,
  "relevance_score": 0-100,
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
  "region": "ì§€ì—­ëª…" or null,
  "has_price": true/false,
  "has_policy": true/false,
  "reason": "íŒë‹¨ ê·¼ê±° 1-2ì¤„"
}"""

    user_prompt = f"""ì œëª©: {title}
ì„¤ëª…: {description}

ì´ ê¸°ì‚¬ê°€ ë¶€ë™ì‚°ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ?"""

    try:
        openai_client_filter = OpenAI(api_key=OPENAI_API_KEY)
        response = openai_client_filter.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"},
            timeout=10
        )
        
        result = json.loads(response.choices[0].message.content)
        
        status = "âœ… ê´€ë ¨" if result['is_relevant'] else "âŒ ë¬´ê´€"
        logging.info(f"{status} (ì ìˆ˜: {result['relevance_score']}) - {title[:40]}...")
        
        return result
        
    except Exception as e:
        logging.error(f"âŒ GPT í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return filter_by_keywords(title, description)

def filter_by_keywords(title: str, description: str) -> dict:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ í•„í„°ë§ (GPT ì‹¤íŒ¨ ì‹œ í´ë°±)"""
    text = (title + " " + description).lower()
    
    real_estate_keywords = [
        "ì•„íŒŒíŠ¸", "ì˜¤í”¼ìŠ¤í…”", "ë¹Œë”©", "ìƒê°€", "í† ì§€", "ì£¼íƒ",
        "ë§¤ë§¤", "ì „ì„¸", "ì›”ì„¸", "ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼",
        "ì¬ê±´ì¶•", "ì¬ê°œë°œ", "ì •ë¹„êµ¬ì—­", "ë¶€ë™ì‚°", "ì§‘ê°’",
        "ì£¼íƒê°€ê²©", "ì „ì„¸ê°€", "ì‹œì„¸", "ì£¼ë‹´ëŒ€", "ì¢…ë¶€ì„¸",
        "ì–‘ë„ì„¸", "ì·¨ë“ì„¸", "êµ­í† ë¶€", "ë¯¸ë¶„ì–‘"
    ]
    
    exclude_keywords = ["ì£¼ì‹", "ì½”ì¸", "ë¹„íŠ¸ì½”ì¸", "í€ë“œ", "ì±„ê¶Œ"]
    
    matched = sum(1 for kw in real_estate_keywords if kw in text)
    excluded = sum(1 for kw in exclude_keywords if kw in text)
    
    score = max(0, min(100, matched * 30 - excluded * 20))
    is_relevant = score >= 30
    
    keywords = [kw for kw in real_estate_keywords if kw in text][:5]
    region = extract_region(text)
    
    logging.info(f"ğŸ”‘ í‚¤ì›Œë“œ í•„í„° (ì ìˆ˜: {score}) - {title[:40]}...")
    
    return {
        'is_relevant': is_relevant,
        'relevance_score': score,
        'keywords': keywords,
        'region': region,
        'has_price': any(kw in text for kw in ['ê°€ê²©', 'ì‹œì„¸', 'ì–µ', 'ë§Œì›', 'ìƒìŠ¹', 'í•˜ë½']),
        'has_policy': any(kw in text for kw in ['ì •ì±…', 'ê·œì œ', 'ì„¸ê¸ˆ', 'ëŒ€ì¶œ', 'ê¸ˆë¦¬']),
        'reason': f'í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ({matched}ê°œ ë§¤ì¹­)'
    }

def extract_region(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ì§€ì—­ ì •ë³´ ì¶”ì¶œ"""
    seoul_gu = [
        "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬",
        "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ë…¸ì›êµ¬", "ë„ë´‰êµ¬",
        "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬",
        "ì„±ë™êµ¬", "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬",
        "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
    ]
    
    gyeonggi_cities = [
        "ì„±ë‚¨ì‹œ", "ìš©ì¸ì‹œ", "ìˆ˜ì›ì‹œ", "ê³ ì–‘ì‹œ", "í™”ì„±ì‹œ",
        "í‰íƒì‹œ", "ë¶€ì²œì‹œ", "ì•ˆì–‘ì‹œ", "ë‚¨ì–‘ì£¼ì‹œ"
    ]
    
    metropolitan = ["ì¸ì²œ", "ë¶€ì‚°", "ëŒ€êµ¬", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ì„¸ì¢…"]
    
    for gu in seoul_gu:
        if gu in text:
            return f"ì„œìš¸ {gu}"
    
    for city in gyeonggi_cities:
        if city in text:
            return f"ê²½ê¸° {city}"
    
    for metro in metropolitan:
        if metro in text:
            return metro
    
    return None

def filter_news_batch(news_items: list) -> list:
    """ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°°ì¹˜ë¡œ í•„í„°ë§ (75ì  ì´ìƒë§Œ)"""
    filtered = []
    
    # í•„í„°ë§ í†µê³„
    filter_stats = {
        'headline': 0,
        'celebrity_scandal': 0,
        'low_score': 0,
        'not_relevant': 0
    }
    
    for item in news_items:
        result = filter_real_estate_news(item['title'], item['description'])
        item.update(result)
        
        # ë¶€ë™ì‚° ê´€ë ¨ + 75ì  ì´ìƒë§Œ í†µê³¼
        if result['is_relevant'] and result.get('relevance_score', 0) >= 75:
            filtered.append(item)
        else:
            # ì œì™¸ ì´ìœ  ì¹´ìš´íŠ¸
            reason = result.get('reason', '').lower()
            if 'í—¤ë“œë¼ì¸' in reason or 'ì¢…í•©' in reason:
                filter_stats['headline'] += 1
            elif 'ì—°ì˜ˆì¸' in reason and 'ë¶„ìŸ' in reason:
                filter_stats['celebrity_scandal'] += 1
            elif result.get('relevance_score', 0) < 75:
                filter_stats['low_score'] += 1
            else:
                filter_stats['not_relevant'] += 1
    
    # í†µê³„ ë¡œê¹…
    total_filtered = sum(filter_stats.values())
    if total_filtered > 0:
        logger.info("")
        logger.info("ğŸ“Š í•„í„°ë§ ì œì™¸ í†µê³„:")
        if filter_stats['headline'] > 0:
            logger.info(f"   - í—¤ë“œë¼ì¸ ë‰´ìŠ¤: {filter_stats['headline']}ê°œ")
        if filter_stats['celebrity_scandal'] > 0:
            logger.info(f"   - ì—°ì˜ˆì¸ ë¶„ìŸ: {filter_stats['celebrity_scandal']}ê°œ")
        if filter_stats['low_score'] > 0:
            logger.info(f"   - ë‚®ì€ ì ìˆ˜ (75ì  ë¯¸ë§Œ): {filter_stats['low_score']}ê°œ")
        if filter_stats['not_relevant'] > 0:
            logger.info(f"   - ë¶€ë™ì‚° ë¬´ê´€: {filter_stats['not_relevant']}ê°œ")
    
    return filtered

# ================================================================================
# ë‰´ìŠ¤ ê²€ìƒ‰
# ================================================================================

def search_naver_news(query: str = "ë¶€ë™ì‚°", display: int = 10) -> Optional[list]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰ + ë¶€ë™ì‚° ê´€ë ¨ì„± í•„í„°ë§"""
    url = "https://openapi.naver.com/v1/search/news.json"
    
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    
    params = {
        "query": query,
        "display": display,
        "sort": "date"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()
        
        items = data.get('items', [])
        if not items:
            return None
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë„ë©”ì¸ë§Œ í•„í„°ë§
        naver_items = [item for item in items if 'news.naver.com' in item['link']]
        
        if not naver_items:
            logger.warning("âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            naver_items = items
        
        logger.info(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ {len(naver_items)}ê°œ ë°œê²¬")
        
        # ëª¨ë“  ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬
        processed_items = []
        for item in naver_items:
            # HTML íƒœê·¸ ì œê±°
            title = re.sub('<[^<]+?>', '', item['title'])
            description = re.sub('<[^<]+?>', '', item['description'])
            
            # HTML ì—”í‹°í‹° ë””ì½”ë”©
            import html
            title = html.unescape(title)
            description = html.unescape(description)
            
            # ìš”ì•½ ê¸¸ì´ ì œí•œ (200ì)
            if len(description) > 200:
                cut_pos = 200
                for i in range(200, max(0, len(description) - 100), -1):
                    if description[i] in '.!?':
                        cut_pos = i + 1
                        break
                description = description[:cut_pos].strip()
            
            processed_items.append({
                "title": title,
                "description": description,
                "link": item['link'],
                "pubDate": item['pubDate'],
                "timestamp": datetime.now().isoformat()
            })
        
        # ë¶€ë™ì‚° ê´€ë ¨ì„± í•„í„°ë§ (75ì  ì´ìƒë§Œ)
        logger.info(f"ğŸ” í•„í„°ë§ ì‹œì‘: {len(processed_items)}ê°œ ê¸°ì‚¬")
        filtered_items = filter_news_batch(processed_items)
        logger.info(
            f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(processed_items)}ê°œ ì¤‘ {len(filtered_items)}ê°œ ì„ ì • (75ì  ì´ìƒ) "
            f"({len(filtered_items)/len(processed_items)*100:.1f}%)"
        )
        return filtered_items
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def crawl_news_content(url: str) -> str:
    """ë‰´ìŠ¤ URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ì¬ì‹œë„ í¬í•¨)"""
    max_retries = 2
    
    for attempt in range(max_retries):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Referer': 'https://news.naver.com/'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            if 'news.naver.com' in url:
                article = soup.select_one('#dic_area') or soup.select_one('#articeBody') or soup.select_one('.news_end')
                if article:
                    for tag in article.find_all(['script', 'style', 'aside']):
                        tag.decompose()
                    content = article.get_text(strip=True, separator='\n')
                    logger.info(f"ğŸ“„ í¬ë¡¤ë§ ì„±ê³µ: {len(content)}ì")
                    return content
            
            # ì¼ë°˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            paragraphs = soup.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
            
            if content:
                logger.info(f"ğŸ“„ í¬ë¡¤ë§ ì„±ê³µ: {len(content)}ì")
                return content
            else:
                return "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                logger.warning(f"âš ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ - ì¬ì‹œë„ {attempt + 1}/{max_retries}")
                time.sleep(2)
                continue
            else:
                logger.error(f"âŒ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ: {url[:50]}...")
                return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (íƒ€ì„ì•„ì›ƒ)"
                
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                if attempt < max_retries - 1:
                    wait_time = 3
                    logger.warning(f"âš ï¸ Rate Limit (429) - {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ Rate Limit ì´ˆê³¼: {url[:50]}...")
                    return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Rate Limit)"
            else:
                logger.error(f"âŒ HTTP ì˜¤ë¥˜ {e.response.status_code}: {url[:50]}...")
                return f"ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (HTTP {e.response.status_code})"
                
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ================================================================================
# Google Sheets & CSV ì €ì¥
# ================================================================================

def init_google_sheets():
    """Initialize Google Sheets client"""
    global gsheet_client, gsheet_worksheet
    
    if not GSPREAD_AVAILABLE:
        logger.error("âŒ gspread not installed")
        return False
    
    if not GOOGLE_SHEETS_CREDENTIALS or not GOOGLE_SHEETS_SPREADSHEET_ID:
        logger.error("âŒ Google Sheets í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •")
        return False
    
    try:
        logger.info("ğŸ”„ Initializing Google Sheets...")
        
        creds_dict = json.loads(GOOGLE_SHEETS_CREDENTIALS)
        scopes = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]
        credentials = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        gsheet_client = gspread.authorize(credentials)
        
        spreadsheet = gsheet_client.open_by_key(GOOGLE_SHEETS_SPREADSHEET_ID)
        gsheet_worksheet = spreadsheet.sheet1
        
        # í—¤ë” í™•ì¸ ë° ìƒì„±
        try:
            headers = gsheet_worksheet.row_values(1)
            if not headers or headers[0] != 'timestamp':
                gsheet_worksheet.insert_row([
                    'timestamp', 'title', 'description', 'url',
                    'is_relevant', 'relevance_score', 'keywords', 'region',
                    'has_price', 'has_policy', 'reason', 'user_id'
                ], 1)
                logger.info("âœ… Google Sheets headers created")
        except:
            gsheet_worksheet.insert_row([
                'timestamp', 'title', 'description', 'url',
                'is_relevant', 'relevance_score', 'keywords', 'region',
                'has_price', 'has_policy', 'reason', 'user_id'
            ], 1)
        
        logger.info(f"âœ… Google Sheets initialized")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Google Sheets: {e}")
        return False

def get_recent_urls_from_gsheet(hours: int = 3) -> set:
    """
    êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ìµœê·¼ Nì‹œê°„ ë‚´ ì €ì¥ëœ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        hours: ëª‡ ì‹œê°„ ì´ë‚´ ë°ì´í„°ë¥¼ í™•ì¸í• ì§€ (ê¸°ë³¸ 3ì‹œê°„)
    
    Returns:
        ìµœê·¼ Nì‹œê°„ ë‚´ URL ì§‘í•©
    """
    global gsheet_worksheet
    
    if not gsheet_worksheet:
        logger.warning("âš ï¸ Google Sheets not initialized - ì¤‘ë³µ ì²´í¬ ë¶ˆê°€")
        return set()
    
    try:
        from datetime import datetime, timedelta
        
        # í˜„ì¬ ì‹œê°„ - Nì‹œê°„
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # ì „ì²´ ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸°
        all_records = gsheet_worksheet.get_all_records()
        
        recent_urls = set()
        
        for record in all_records:
            try:
                # timestamp íŒŒì‹± (ISO format)
                timestamp_str = record.get('timestamp', '')
                if not timestamp_str:
                    continue
                
                # ISO format íŒŒì‹±
                record_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                
                # ìµœê·¼ Nì‹œê°„ ì´ë‚´ë©´ URL ì¶”ê°€
                if record_time >= cutoff_time:
                    url = record.get('url', '')
                    if url:
                        recent_urls.add(url)
            except Exception as e:
                # ê°œë³„ ë ˆì½”ë“œ íŒŒì‹± ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                continue
        
        logger.info(f"ğŸ“‹ ìµœê·¼ {hours}ì‹œê°„ URL í™•ì¸: {len(recent_urls)}ê°œ")
        return recent_urls
        
    except Exception as e:
        logger.error(f"âŒ ìµœê·¼ URL ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return set()

def init_csv_file():
    """Initialize CSV file with headers"""
    try:
        if not os.path.exists(CSV_FILE_PATH):
            with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp', 'title', 'description', 'url',
                    'is_relevant', 'relevance_score', 'keywords', 'region',
                    'has_price', 'has_policy', 'reason', 'user_id'
                ])
            logger.info(f"âœ… CSV file created: {CSV_FILE_PATH}")
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to initialize CSV: {e}")
        return False

def save_news_to_csv(news_data: dict):
    """Save news to CSV file"""
    try:
        with open(CSV_FILE_PATH, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                news_data['timestamp'],
                news_data['title'],
                news_data['description'],
                news_data['url'],
                news_data.get('is_relevant', True),
                news_data.get('relevance_score', 0),
                ', '.join(news_data.get('keywords', [])),
                news_data.get('region', ''),
                news_data.get('has_price', False),
                news_data.get('has_policy', False),
                news_data.get('reason', ''),
                news_data['user_id']
            ])
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to save to CSV: {e}")
        return False

def save_news_to_gsheet(news_data: dict):
    """Save news to Google Sheets"""
    if not gsheet_worksheet:
        logger.warning("âš ï¸ Google Sheets not initialized - skipping")
        return False
    
    try:
        gsheet_worksheet.append_row([
            news_data['timestamp'],
            news_data['title'],
            news_data['description'],
            news_data['url'],
            news_data.get('is_relevant', True),
            news_data.get('relevance_score', 0),
            ', '.join(news_data.get('keywords', [])),
            news_data.get('region', ''),
            news_data.get('has_price', False),
            news_data.get('has_policy', False),
            news_data.get('reason', ''),
            news_data['user_id']
        ])
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to save to Google Sheets: {e}")
        return False

async def save_all_news_background(news_items: list, user_id: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë“  ë‰´ìŠ¤ ì €ì¥ (í¬ë¡¤ë§ ì—†ì´ ë©”íƒ€ë°ì´í„°ë§Œ)"""
    logger.info(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ì €ì¥ ì‹œì‘: {len(news_items)}ê°œ (í¬ë¡¤ë§ ì œì™¸)")
    saved_count = 0
    
    for idx, news_item in enumerate(news_items):
        try:
            if idx > 0:
                await asyncio.sleep(0.5)
            
            # í‚¤ ì´ë¦„ í†µì¼ (link â†’ url)
            if 'link' in news_item and 'url' not in news_item:
                news_item['url'] = news_item['link']
            
            news_item['user_id'] = user_id
            
            # í•„í„°ë§ ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’
            if 'is_relevant' not in news_item:
                news_item['is_relevant'] = True
                news_item['relevance_score'] = 50
                news_item['keywords'] = []
                news_item['region'] = ''
                news_item['has_price'] = False
                news_item['has_policy'] = False
                news_item['reason'] = 'Filtering module not available'
            
            # ì €ì¥
            save_news_to_csv(news_item)
            save_news_to_gsheet(news_item)
            
            saved_count += 1
            logger.info(
                f"âœ… [{saved_count}/{len(news_items)}] ì €ì¥ ì™„ë£Œ "
                f"[{news_item.get('relevance_score', 0)}ì ] "
                f"{news_item['title'][:30]}..."
            )
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ {idx+1} ì €ì¥ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"ğŸ‰ ë°±ê·¸ë¼ìš´ë“œ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
